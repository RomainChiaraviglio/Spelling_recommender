{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spelling Recommender\n",
    "\n",
    "Creation of two spelling correctors/recommenders using the Jaccard distance and the edit distance methods\n",
    "\n",
    "Each of the recommenders provide recommendations for the three default words : `['cormulent', 'incendenece', 'validrate']`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading collection 'popular'\n",
      "[nltk_data]    | \n",
      "[nltk_data]    | Downloading package cmudict to\n",
      "[nltk_data]    |     C:\\Users\\chiar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\cmudict.zip.\n",
      "[nltk_data]    | Downloading package gazetteers to\n",
      "[nltk_data]    |     C:\\Users\\chiar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\gazetteers.zip.\n",
      "[nltk_data]    | Downloading package genesis to\n",
      "[nltk_data]    |     C:\\Users\\chiar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\genesis.zip.\n",
      "[nltk_data]    | Downloading package gutenberg to\n",
      "[nltk_data]    |     C:\\Users\\chiar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\gutenberg.zip.\n",
      "[nltk_data]    | Downloading package inaugural to\n",
      "[nltk_data]    |     C:\\Users\\chiar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\inaugural.zip.\n",
      "[nltk_data]    | Downloading package movie_reviews to\n",
      "[nltk_data]    |     C:\\Users\\chiar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\movie_reviews.zip.\n",
      "[nltk_data]    | Downloading package names to\n",
      "[nltk_data]    |     C:\\Users\\chiar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\names.zip.\n",
      "[nltk_data]    | Downloading package shakespeare to\n",
      "[nltk_data]    |     C:\\Users\\chiar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\shakespeare.zip.\n",
      "[nltk_data]    | Downloading package stopwords to\n",
      "[nltk_data]    |     C:\\Users\\chiar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\stopwords.zip.\n",
      "[nltk_data]    | Downloading package treebank to\n",
      "[nltk_data]    |     C:\\Users\\chiar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\treebank.zip.\n",
      "[nltk_data]    | Downloading package twitter_samples to\n",
      "[nltk_data]    |     C:\\Users\\chiar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\twitter_samples.zip.\n",
      "[nltk_data]    | Downloading package omw to\n",
      "[nltk_data]    |     C:\\Users\\chiar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\omw.zip.\n",
      "[nltk_data]    | Downloading package wordnet to\n",
      "[nltk_data]    |     C:\\Users\\chiar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\wordnet.zip.\n",
      "[nltk_data]    | Downloading package wordnet_ic to\n",
      "[nltk_data]    |     C:\\Users\\chiar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\wordnet_ic.zip.\n",
      "[nltk_data]    | Downloading package words to\n",
      "[nltk_data]    |     C:\\Users\\chiar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\words.zip.\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
      "[nltk_data]    |     C:\\Users\\chiar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping chunkers\\maxent_ne_chunker.zip.\n",
      "[nltk_data]    | Downloading package punkt to\n",
      "[nltk_data]    |     C:\\Users\\chiar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping tokenizers\\punkt.zip.\n",
      "[nltk_data]    | Downloading package snowball_data to\n",
      "[nltk_data]    |     C:\\Users\\chiar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]    |     C:\\Users\\chiar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping taggers\\averaged_perceptron_tagger.zip.\n",
      "[nltk_data]    | \n",
      "[nltk_data]  Done downloading collection popular\n",
      "[nltk_data] Downloading package nps_chat to\n",
      "[nltk_data]     C:\\Users\\chiar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\nps_chat.zip.\n",
      "[nltk_data] Downloading package webtext to\n",
      "[nltk_data]     C:\\Users\\chiar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\webtext.zip.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('popular')\n",
    "nltk.download('nps_chat')\n",
    "nltk.download('webtext')\n",
    "from nltk.corpus import words\n",
    "\n",
    "correct_spellings = words.words()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jaccard distance\n",
    "\n",
    "For this recommender, we will use the following distance metric:\n",
    "\n",
    "**[Jaccard distance](https://en.wikipedia.org/wiki/Jaccard_index) on the trigrams of the two words (word in entry and word in the nltk corpus).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corpulent indecence validate\n",
      "execution time : 0.4411618709564209 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "entries=['cormulent', 'incendenece', 'validrate']\n",
    "\n",
    "# Timing the execution\n",
    "start_time = time.time()\n",
    "\n",
    "cormu_tri = set(nltk.ngrams('cormulent', n=3))\n",
    "incen_tri = set(nltk.ngrams('incendenece', n=3))\n",
    "vali_tri = set(nltk.ngrams('validrate', n=3))\n",
    "    \n",
    "dist_cormu = 1\n",
    "dist_incen = 1\n",
    "dist_vali = 1\n",
    "    \n",
    "for x in correct_spellings:\n",
    "    if x[0] == entries[0][0] and nltk.jaccard_distance(cormu_tri, set(nltk.ngrams(x, n=3))) < dist_cormu:\n",
    "        min_cormu = x\n",
    "        dist_cormu = nltk.jaccard_distance(cormu_tri, set(nltk.ngrams(x, n=3)))\n",
    "            \n",
    "    if x[0] == entries[1][0] and nltk.jaccard_distance(incen_tri, set(nltk.ngrams(x, n=3))) < dist_incen:\n",
    "        min_incen = x\n",
    "        dist_incen = nltk.jaccard_distance(incen_tri, set(nltk.ngrams(x, n=3)))\n",
    "            \n",
    "    if x[0] == entries[2][0] and nltk.jaccard_distance(vali_tri, set(nltk.ngrams(x, n=3))) < dist_vali:\n",
    "        min_vali = x\n",
    "        dist_vali = nltk.jaccard_distance(vali_tri, set(nltk.ngrams(x, n=3)))\n",
    "\n",
    "    \n",
    "print(min_cormu, min_incen, min_vali)\n",
    "print(\"execution time : %s seconds\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Edit distance\n",
    "\n",
    "For this recommender, we will use the following distance metric:\n",
    "\n",
    "**[Edit distance on the two words with transpositions.](https://en.wikipedia.org/wiki/Damerau%E2%80%93Levenshtein_distance)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corpulent intendence validate\n",
      "execution time : 6.542162179946899 seconds\n"
     ]
    }
   ],
   "source": [
    "# Timing the execution\n",
    "start_time = time.time()\n",
    "\n",
    "dist_cormu = 50\n",
    "dist_incen = 50\n",
    "dist_vali = 50\n",
    "    \n",
    "for x in correct_spellings:\n",
    "    if x[0] == entries[0][0] and nltk.edit_distance('cormulent', x, transpositions=True) < dist_cormu:\n",
    "        min_cormu = x\n",
    "        dist_cormu = nltk.edit_distance('cormulent', x, transpositions=True)\n",
    "            \n",
    "    if x[0] == entries[1][0] and nltk.edit_distance('incendenece', x, transpositions=True) < dist_incen:\n",
    "        min_incen = x\n",
    "        dist_incen = nltk.edit_distance('incendenece', x, transpositions=True)\n",
    "            \n",
    "    if x[0] == entries[2][0] and nltk.edit_distance('validrate', x, transpositions=True) < dist_vali:\n",
    "        min_vali = x\n",
    "        dist_vali = nltk.edit_distance('validrate', x, transpositions=True)\n",
    "\n",
    "print(min_cormu, min_incen, min_vali)\n",
    "print(\"execution time : %s seconds\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Conclusion\n",
    "\n",
    "The jaccard distance method is way faster than the edit distance. The results are almost the same, the edit distance results are maybe a little more accurate : intendence is closer to incendenece than indecence.\n",
    "\n",
    "Finally for a live spelling corrector use, the jaccard distance seems to be a better choice. But for an accurate use like a text verification, the edit distance may be better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "coursera": {
   "course_slug": "python-text-mining",
   "graded_item_id": "r35En",
   "launcher_item_id": "tCVfW",
   "part_id": "NTVgL"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
